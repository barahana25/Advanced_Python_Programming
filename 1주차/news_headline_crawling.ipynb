{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b967086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "# User-Agent ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)\n",
    "ua = UserAgent()\n",
    "\n",
    "# ê¸°ë³¸ í—¤ë” ì„¤ì •\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',\n",
    "    'Accept-Encoding': 'gzip, deflate',\n",
    "    'Connection': 'keep-alive',\n",
    "}\n",
    "\n",
    "# ì„¸ì…˜ ìƒì„± (ì—°ê²° ì¬ì‚¬ìš©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ)\n",
    "session = requests.Session()\n",
    "session.headers.update(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4219a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4be696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 82ê°œì˜ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìˆ˜ì§‘ ì™„ë£Œ!\n",
      "âœ… ë°ì´í„°ê°€ news_headlines_20250910_092943.csvì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\n",
      "                                           title  \\\n",
      "0  ê¹€ìƒë¯¼ ì „ ê²€ì‚¬, 13ì‹œê°„ íŠ¹ê²€ ì¡°ì‚¬ ë’¤ ê·€ê°€ \"ê¹€ê±´í¬ ì˜¤ë¹  ìš”ì²­ìœ¼ë¡œ ê·¸ë¦¼ ì¤‘ê°œ\"   \n",
      "1             [ë‹¨ë…]'í•œí•™ì ì†Œí™˜ ì„ë°•' í†µì¼êµ, ì „ì„¸ê³„ ê°„ë¶€ ì „ì› ì†Œì§‘ë ¹   \n",
      "2          ê¶Œì„±ë™ \"ï§¡ëŒ€í†µë ¹ 'í•„ë¦¬í•€ ì°¨ê´€ì‚¬ì—…' íŒŒê¸°, æ–‡ê³¼ ê°™ì€ ì •ì  íƒ„ì••\"   \n",
      "3             ì¢ŒíŒŒ ì •ì¹˜ì¸ë“¤ì˜ í•œì—†ì´ ëª»ëœ ë§ë²„ë¦‡ [ì´ì§„ê³¤ì˜ ê·¸ê±´ ì•„ë‹ˆì§€ìš”]   \n",
      "4                            ëˆ„êµ°ê°€ â€œë‚˜ë¥¼ ì£½ì—¬ë‹¬ë¼â€ ë¶€íƒí•œë‹¤ë©´   \n",
      "\n",
      "                                               url           crawl_time  \n",
      "0  https://n.news.naver.com/article/657/0000042627  2025-09-10 09:29:43  \n",
      "1  https://n.news.naver.com/article/079/0004064411  2025-09-10 09:29:43  \n",
      "2  https://n.news.naver.com/article/088/0000969236  2025-09-10 09:29:43  \n",
      "3  https://n.news.naver.com/article/119/0003000788  2025-09-10 09:29:43  \n",
      "4  https://n.news.naver.com/article/036/0000052277  2025-09-10 09:29:43  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def crawl_news_headlines():\n",
    "    \"\"\"\n",
    "    ë‰´ìŠ¤ í—¤ë“œë¼ì¸ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "    (ì˜ˆì‹œ: ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ì˜ˆì‹œ URL (ì‹¤ì œë¡œëŠ” í—ˆê°€ëœ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì‚¬ìš©)\n",
    "    base_url = \"https://news.naver.com\"  # í…ŒìŠ¤íŠ¸ìš© URL\n",
    "    \n",
    "    news_data = []\n",
    "    \n",
    "    try:\n",
    "        # í˜ì´ì§€ ìš”ì²­\n",
    "        response = session.get(base_url)\n",
    "        response.raise_for_status()  # HTTP ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # ì‹¤ì œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì˜ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •\n",
    "        # ì˜ˆì‹œ: headlines = soup.find_all('h2', class_='headline')\n",
    "        headlines = soup.find_all('a', class_='cnf_news_area _cds_link _editn_link')\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±\n",
    "        # headlines = [\"AI ê¸°ìˆ  ë°œì „ ê°€ì†í™”\", \"íŒŒì´ì¬ ì¸ê¸° ì§€ì† ìƒìŠ¹\", \"ë°ì´í„° ë¶„ì„ ì‹œì¥ í™•ëŒ€\"]\n",
    "        \n",
    "        for i, headline in enumerate(headlines, 1):\n",
    "            url = headline['href']\n",
    "            headline = headline.find('strong').get_text(strip=True)\n",
    "            news_item = {\n",
    "                'title': headline,\n",
    "                'url': url,\n",
    "                'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            news_data.append(news_item)\n",
    "            \n",
    "        print(f\"âœ… {len(news_data)}ê°œì˜ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìˆ˜ì§‘ ì™„ë£Œ!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    \n",
    "    return news_data\n",
    "\n",
    "def save_news_data(news_data):\n",
    "    \"\"\"ìˆ˜ì§‘í•œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "    if news_data:\n",
    "        df = pd.DataFrame(news_data)\n",
    "        filename = f\"news_headlines_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        df.to_csv(filename, index=True, encoding='utf-8-sig')\n",
    "        print(f\"âœ… ë°ì´í„°ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "# ì‹¤í–‰\n",
    "news_data = crawl_news_headlines()\n",
    "df = save_news_data(news_data)\n",
    "if df is not None:\n",
    "    print(\"\\nğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5398ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
