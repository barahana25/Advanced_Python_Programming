{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b967086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "# User-Agent 설정 (봇 차단 방지)\n",
    "ua = UserAgent()\n",
    "\n",
    "# 기본 헤더 설정\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',\n",
    "    'Accept-Encoding': 'gzip, deflate',\n",
    "    'Connection': 'keep-alive',\n",
    "}\n",
    "\n",
    "# 세션 생성 (연결 재사용으로 성능 향상)\n",
    "session = requests.Session()\n",
    "session.headers.update(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4219a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4be696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 82개의 뉴스 헤드라인 수집 완료!\n",
      "✅ 데이터가 news_headlines_20250910_092943.csv에 저장되었습니다.\n",
      "\n",
      "📊 수집된 데이터 미리보기:\n",
      "                                           title  \\\n",
      "0  김상민 전 검사, 13시간 특검 조사 뒤 귀가 \"김건희 오빠 요청으로 그림 중개\"   \n",
      "1             [단독]'한학자 소환 임박' 통일교, 전세계 간부 전원 소집령   \n",
      "2          권성동 \"李대통령 '필리핀 차관사업' 파기, 文과 같은 정적 탄압\"   \n",
      "3             좌파 정치인들의 한없이 못된 말버릇 [이진곤의 그건 아니지요]   \n",
      "4                            누군가 “나를 죽여달라” 부탁한다면   \n",
      "\n",
      "                                               url           crawl_time  \n",
      "0  https://n.news.naver.com/article/657/0000042627  2025-09-10 09:29:43  \n",
      "1  https://n.news.naver.com/article/079/0004064411  2025-09-10 09:29:43  \n",
      "2  https://n.news.naver.com/article/088/0000969236  2025-09-10 09:29:43  \n",
      "3  https://n.news.naver.com/article/119/0003000788  2025-09-10 09:29:43  \n",
      "4  https://n.news.naver.com/article/036/0000052277  2025-09-10 09:29:43  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def crawl_news_headlines():\n",
    "    \"\"\"\n",
    "    뉴스 헤드라인 크롤링 함수\n",
    "    (예시: 실제 사이트 구조에 맞게 수정 필요)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 예시 URL (실제로는 허가된 뉴스 사이트 사용)\n",
    "    base_url = \"https://news.naver.com\"  # 테스트용 URL\n",
    "    \n",
    "    news_data = []\n",
    "    \n",
    "    try:\n",
    "        # 페이지 요청\n",
    "        response = session.get(base_url)\n",
    "        response.raise_for_status()  # HTTP 에러 발생 시 예외 처리\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # 실제 뉴스 사이트의 구조에 맞게 수정\n",
    "        # 예시: headlines = soup.find_all('h2', class_='headline')\n",
    "        headlines = soup.find_all('a', class_='cnf_news_area _cds_link _editn_link')\n",
    "        \n",
    "        # 테스트용 데이터 생성\n",
    "        # headlines = [\"AI 기술 발전 가속화\", \"파이썬 인기 지속 상승\", \"데이터 분석 시장 확대\"]\n",
    "        \n",
    "        for i, headline in enumerate(headlines, 1):\n",
    "            url = headline['href']\n",
    "            headline = headline.find('strong').get_text(strip=True)\n",
    "            news_item = {\n",
    "                'title': headline,\n",
    "                'url': url,\n",
    "                'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            news_data.append(news_item)\n",
    "            \n",
    "        print(f\"✅ {len(news_data)}개의 뉴스 헤드라인 수집 완료!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 크롤링 중 오류 발생: {e}\")\n",
    "    \n",
    "    return news_data\n",
    "\n",
    "def save_news_data(news_data):\n",
    "    \"\"\"수집한 뉴스 데이터를 CSV 파일로 저장\"\"\"\n",
    "    if news_data:\n",
    "        df = pd.DataFrame(news_data)\n",
    "        filename = f\"news_headlines_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        df.to_csv(filename, index=True, encoding='utf-8-sig')\n",
    "        print(f\"✅ 데이터가 {filename}에 저장되었습니다.\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"❌ 저장할 데이터가 없습니다.\")\n",
    "        return None\n",
    "\n",
    "# 실행\n",
    "news_data = crawl_news_headlines()\n",
    "df = save_news_data(news_data)\n",
    "if df is not None:\n",
    "    print(\"\\n📊 수집된 데이터 미리보기:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5398ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
